# Micro-Courses: Learning Outcomes & Expectations Framework

## Overview
These 10 courses teach teachers how to use AI effectively in PBL classrooms. They're organized into 3 tiers, each building toward a concrete deliverable:
- **Tier 1**: Build an AI Classroom Constitution (foundational document)
- **Tier 2**: Build a complete PBL unit using the Constitution
- **Tier 3**: Build a Classroom AI Policy (norms + activities for student AI use)

---

## TIER 1: AI CLASSROOM CONSTITUTION

### Course 1: Constraints
**What it teaches**: AI needs specificity to be useful. Generic prompts → generic outputs. Constraints are how you tell AI what makes *your* classroom unique.

**Learning outcome**: Teachers can identify and articulate the 5 key constraints (project phase, prior work, time box, differentiation, final product connection).

**Deliverable**: First two sections of the Constitution:
- Section 1: Classroom Context (grade, class size, reading levels, EL/IEP needs, scaffolds, period length, district requirements)
- Section 2: Project Architecture (project topic, driving question, timeline, milestones completed, what's next, final product, audience, disciplines)

**Prerequisites**: None (entry point)

**Assessment**:
- CFU-1: Prompt comparison (choose the constrained prompt over the generic one)
- CFU-2: Identify-missing (spot 3+ missing constraints in a weak rubric prompt)
- Workshop: Actually write their own Constitution Section 1 & 2 and test it with real AI

**Common misconceptions to address**:
- "More words = better constraints" (quality over quantity)
- "This is extra work" (actually saves time by reducing iterations)
- "I only need to do this once" (Constitution evolves as project evolves)

**Connection to Role Assignment**: Role Assignment assumes the teacher has already defined their classroom context and project. The Constitution is the input.

---

### Course 2: Role Assignment
**What it teaches**: When you assign AI a role or persona, you activate different patterns of expertise and perspective. This makes feedback more authentic and prepares students for real stakeholder interaction.

**Learning outcome**: Teachers can identify key stakeholders in their project and write role prompts that simulate authentic perspectives (skeptical funding officer, community partner, content expert).

**Deliverable**: Section 3 of the Constitution:
- Stakeholder & Perspective Map: "The people who matter in this project are... When we need [type of feedback], we ask AI to roleplay as..."
- 3-5 specific role prompts ready to drop into the Constitution

**Prerequisites**: Constraints course (uses Constitution sections 1 & 2 as context)

**Assessment**:
- CFU-1: Prompt comparison (constrained role vs. generic feedback request)
- CFU-2: Design a role prompt for a specific stakeholder
- Workshop: Map their actual stakeholders and write 3 role prompts they can reuse

**Common misconceptions to address**:
- "Roles work better in some AI tools than others" (teaching that constraints matter more than tool choice)
- "A role is like a character I make up" (it's pattern-matching to real expertise/concerns)
- "I only need one role" (different people see projects differently)

**Connection to Iteration**: Iteration picks up the completed Constitution (3 sections so far) and teaches teachers how to refine it based on actual AI outputs.

---

### Course 3: Iteration
**What it teaches**: Your first prompt is a prototype, not a final product. Use critique cycles to steer AI toward better outputs instead of starting over from scratch.

**Learning outcome**: Teachers can generate, critique, and refine AI outputs using a structured protocol. They understand the difference between "start from scratch" and "give feedback."

**Deliverable**: Section 4 of the Constitution:
- Quality Standards: "For my classroom, 'ready' means... I'll know an output is good when... If it's not working, here's how I critique it..."
- Refined versions of Sections 1-3 based on real testing

**Prerequisites**: Constraints + Role Assignment (uses full Constitution so far)

**Assessment**:
- CFU-1: Critique protocol (given a mediocre AI output, identify what to ask AI to fix)
- CFU-2: Iteration plan (given feedback from a teacher, rewrite the prompt)
- Workshop: Test their Constitution with actual AI (in Claude, Gemini, etc.) and iterate until they get outputs they'd actually use in class

**Common misconceptions to address**:
- "If the output is bad, I need a better prompt" (often just needs targeted refinement)
- "Iteration takes longer than starting over" (teach that it's usually faster)
- "I should accept the first output or ask for a complete redo" (the middle ground of critique is missing)

**Connection to Tier 2**: At the end of Tier 1, teachers have a complete, tested AI Classroom Constitution. Tier 2 uses that Constitution as the input to build an entire PBL unit.

---

## TIER 2: COMPLETE PBL UNIT DESIGN

### Course 4: Meta-Prompting
**What it teaches**: Use AI not just to complete tasks, but to improve your prompts before or after you use them. This is "prompt engineering with a feedback loop."

**Learning outcome**: Teachers can use meta-prompts to stress-test their PBL unit ideas. They know how to ask AI "What's missing?" and "How would this fail?" before investing time.

**Deliverable**: A complete PBL unit outline, vetted by AI:
- Driving question (clear and open-ended)
- Project arc (week-by-week or phase-by-phase)
- Assessment (how students show mastery)
- Scaffolds (where students might struggle)
- Connection points (how each phase builds to the next)

**Prerequisites**: Tier 1 complete (especially the Constitution—that's the input)

**Assessment**:
- CFU-1: Meta-prompt comparison (generic design request vs. one that asks AI to find gaps)
- CFU-2: Identify what's missing (given a weak unit outline, spot 3+ gaps)
- Workshop: Load their Constitution into an AI tool, share their unit concept, and use meta-prompts to stress-test it

**Common misconceptions to address**:
- "I should design the unit alone, then let AI critique it" (co-designing with AI is more powerful)
- "Meta-prompting is for experts only" (it's just asking AI to think out loud with you)
- "If AI finds a gap, my unit is bad" (gaps are *expected*—that's why we find them early)

**Connection to Persona Calling**: Persona Calling takes the unit structure from Meta-Prompting and asks: "Who do students need to *be* in this project?" It enriches the unit by adding learner agency.

---

### Course 5: Persona Calling
**What it teaches**: Persona calling isn't about hiding who students are—it's about activating their agency. When students take on a role with real constraints and audience, the work becomes meaningful.

**Learning outcome**: Teachers can design authentic personas for students (scientist, community advocate, engineer, journalist) that connect to the project's real work.

**Deliverable**: Persona section of the unit:
- What role(s) do students adopt? (e.g., "environmental consultants investigating water quality")
- What are their constraints? (What do they *not* know? What pressure are they under?)
- Who is their audience? (Who will evaluate their work?)
- How does this persona connect to the driving question?

**Prerequisites**: Meta-Prompting (uses the unit outline from that course)

**Assessment**:
- CFU-1: Persona comparison (surface persona vs. one with real constraints and audience)
- CFU-2: Write a persona prompt (design a student role for a given project)
- Workshop: Design 1-2 student personas for their actual project and test how AI responses change based on the persona

**Common misconceptions to address**:
- "Persona = costume" (it's about role-based constraints and decision-making)
- "This limits what students can do" (actually, clear constraints *enable* better thinking)
- "Students will be confused if they're in a role" (students *want* clarity about what they're supposed to do)

**Connection to Workflow Design**: Workflow Design takes the unit (with its personas) and asks: "How do students actually *interact* with AI while living in that role?"

---

### Course 6: Workflow Design
**What it teaches**: Workflow design systematizes how students work with AI during a project. It's not "give students AI and hope they use it well"—it's designing explicit, repeatable processes.

**Learning outcome**: Teachers can map the specific moments when students will use AI and how. They create workflow templates students can reuse throughout a project.

**Deliverable**: Workflow section of the unit:
- Specific moments when students use AI (e.g., "Monday: Generate hypotheses with AI, Tuesday: Critique + refine")
- Workflow templates (e.g., "The Research Conversation," "The Feedback Loop," "The Prototype Sprint")
- Student-facing prompts or sentence frames (scaffolds that guide AI use)
- Success criteria (how you know the workflow is working)

**Prerequisites**: Meta-Prompting + Persona Calling (uses the full unit structure so far)

**Assessment**:
- CFU-1: Workflow comparison (ad-hoc vs. systematic AI use)
- CFU-2: Identify workflow gaps (given a project arc, spot where students need support with AI)
- Workshop: Design 2-3 concrete workflows for their project. Test one with actual AI to make sure it produces useful outputs.

**Common misconceptions to address**:
- "Workflows constrain student thinking" (good workflows *enable* independent thinking)
- "Students will game the workflow" (clear structures actually reduce gaming)
- "I need to design a unique workflow for every task" (reusable templates are more sustainable)

**Connection to Tier 3**: At the end of Tier 2, teachers have a complete, AI-integrated PBL unit ready to teach. Tier 3 addresses the bigger question: "How do we help students and our school *think well* about AI as a tool?"

---

## TIER 3: CLASSROOM AI POLICY

### Course 7: Critical Evaluation
**What it teaches**: Students need to know how to evaluate AI outputs, not just generate them. This course teaches teachers how to scaffold critical evaluation so students aren't passive consumers.

**Learning outcome**: Teachers can design prompts and rubrics that help students ask: Is this accurate? Is it original? Does it answer *my* question? Have I thought about this, or is AI doing my thinking?

**Deliverable**: Evaluation frameworks for the classroom:
- Student-facing rubric: "How to critique an AI output" (accuracy, relevance, originality, reasoning clarity)
- Teacher prompts: How to ask students "What's good about this? What's missing? How would you improve it?"
- Integration points: Where in your workflow do students stop and evaluate?

**Prerequisites**: Tier 1 + 2 (understanding constraints, roles, workflows gives context for why evaluation matters)

**Assessment**:
- CFU-1: Framework comparison (surface evaluation vs. one that teaches critical thinking)
- CFU-2: Design an evaluation moment (given a learning scenario, write a prompt that helps students critique AI output)
- Workshop: Create a student-facing rubric for their project and test it with one AI-generated output

**Common misconceptions to address**:
- "Students trust AI too much" (teaching them to evaluate is the antidote)
- "Evaluation slows things down" (built into the right workflow, it's fast and productive)
- "I need to evaluate for them" (student evaluation develops metacognition)

**Connection to Detecting AI Work**: Critical Evaluation teaches students *how* to think about AI outputs. Detecting AI Work addresses the flip side: How do teachers tell when students are *using* AI vs. *thinking with* AI?

---

### Course 8: Detecting AI Work
**What it teaches**: You can't manage what you can't see. This course teaches teachers how to recognize when students are using AI (and whether that's good or not in context).

**Learning outcome**: Teachers can distinguish between: (1) original student work, (2) appropriate AI-assisted work, (3) over-reliance on AI, and (4) off-topic AI-generated filler.

**Deliverable**: Detection & response framework:
- What does each category *look like* in student work? (Examples and non-examples)
- How to have the conversation: "I notice you used AI here. Tell me about your thinking."
- When to intervene vs. celebrate
- Assessment design that makes AI use visible (and intentional)

**Prerequisites**: Critical Evaluation + any Tier 1 or 2 course (context about how AI should work in your classroom)

**Assessment**:
- CFU-1: Sample analysis (read student work, identify what's AI-generated vs. original)
- CFU-2: Response scenario (given a situation, decide whether to celebrate, coach, or redirect)
- Workshop: Look at examples of student work and write a 1:1 conversation starter for ambiguous cases

**Common misconceptions to address**:
- "If I see AI, the student did something wrong" (context matters—sometimes AI use is perfect)
- "I can always tell when AI was used" (you can't, but patterns reveal intention)
- "I have to ban AI use to maintain academic integrity" (transparent, scaffolded use is more honest)

**Connection to Student AI Activities**: Detecting AI Work is about *monitoring*. Student AI Activities is about *designing*—intentionally building AI use into the learning.

---

### Course 9: Student AI Activities
**What it teaches**: AI isn't a shortcut—it's a cognitive tool. This course teaches teachers how to design activities where students use AI *as a learning experience*, not instead of learning.

**Learning outcome**: Teachers can design activities where students use AI to learn research skills, evaluate sources, iterate thinking, get feedback, and develop metacognition.

**Deliverable**: Activity designs for AI-based learning:
- Examples: "The Research Conversation" (interview AI like an expert), "The Feedback Sprint" (get AI feedback, revise, compare), "The Assumption Check" (ask AI where it's uncertain), "The Iteration Studio" (students refine drafts using AI feedback)
- For each activity: learning target, workflow, success criteria, common pitfalls
- How each activity connects to explicit learning outcomes (not just "use AI")

**Prerequisites**: All Tier 2 courses (activities live *inside* workflows designed in Workflow Design)

**Assessment**:
- CFU-1: Activity quality (distinguish between shallow AI use and deep learning design)
- CFU-2: Align activity to learning target (given a target, improve an activity that's currently too loose)
- Workshop: Design one activity for their project where students use AI to *learn*, not shortcut learning

**Common misconceptions to address**:
- "AI activities are for advanced students" (they're especially powerful for struggling learners)
- "Using AI means students don't think" (well-designed activities make thinking visible and necessary)
- "I need an AI activity for every unit" (start small—one solid activity is better than many shallow ones)

**Connection to Curriculum AI Design**: Student AI Activities teach teachers how to leverage AI *within* a unit. Curriculum AI Design asks the bigger question: "How do we redesign a whole course around AI?"

---

### Course 10: Curriculum AI Design
**What it teaches**: This is the capstone. Teachers learn to think systematically about where AI fits in a whole curriculum, not just one project.

**Learning outcome**: Teachers can audit a course/curriculum and redesign it to leverage AI as a cognitive tool where it matters most.

**Deliverable**: A curriculum audit + redesign framework:
- Where in my course do students get stuck? (Research bottleneck, revision aversion, limited feedback, etc.)
- Where could AI help them think better, not just faster?
- What would need to change in my unit design, my assessment, my feedback cycles?
- A 3-5 year AI integration roadmap for the course/curriculum

**Prerequisites**: All Tier 1 and Tier 2, at least one Tier 3 course (you need the foundation before thinking at scale)

**Assessment**:
- CFU-1: Integration comparison (surface "add AI" vs. thoughtful "redesign for AI thinking")
- CFU-2: Curriculum audit (given a course, identify high-leverage places for AI use)
- Workshop: Audit one of their courses. Identify 3 places where AI could support learning. Design how you'd introduce AI there without overwhelming yourself.

**Common misconceptions to address**:
- "Curriculum AI design = every student uses AI constantly" (it's about strategic, purposeful use)
- "I need to redesign everything" (start with high-leverage moments, iterate over time)
- "AI will replace me" (this course is about AI amplifying what you already do)

**Closure/Reflection**:
- You now have all three: A Constitution (Tier 1), a PBL unit design (Tier 2), and a curriculum vision (Tier 3)
- The Constitution is a living document—it evolves as you learn more about what works
- The real work starts now: taking this back to your classroom

---

## CROSS-COURSE PATTERNS TO EMPHASIZE

1. **Accumulation**: Each course adds to previous learning. Tier 1 builds the Constitution. Tier 2 uses it to build a unit. Tier 3 uses both to think about curriculum.

2. **Feedback loops**: Every course has a workshop where teachers *test* their thinking with actual AI. This isn't theoretical.

3. **Constraints = freedom**: Across all courses, the through-line is that specificity and constraints actually *enable* better AI use, not constrain it.

4. **Audience matters**: The Constitution is for AI. The role prompts are for authentic stakeholders. The workflows are for students. Each course clarifies WHO you're communicating with.

5. **Iteration as mindset**: Not just in the Iteration course, but throughout—first draft → test → refine → repeat.

---

## NOTES FOR OPUS PROMPT

When writing the courses, keep these in mind:

- **Each course should *feel* like it's building toward something real.** By the end, teachers have a Constitution. By the end of Tier 2, they have a unit outline. By the end of Tier 3, they have a vision.

- **Use the same example throughout where possible.** The hydroponics project works as a through-line across Constraints, Role Assignment, and Iteration. Use it—don't restart with new examples each course.

- **Connect explicitly.** Don't assume teachers remember what they learned before. Start each course context with: "In [previous course], you built X. Now we're adding Y because..."

- **Make failure visible.** Before-and-after examples aren't enough. Show the *process* of recognizing a weak prompt and improving it.

- **Voice**: Friendly, collaborative, not preachy. "You've probably noticed..." not "Everyone knows..."

